{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNk09d6EqAx3"
      },
      "source": [
        "# ДЗ1. CLAP. Обучение проекции из аудио в текстовое пространство CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQrsmgbkqXVZ"
      },
      "source": [
        "**Описание задания**\n",
        "\n",
        "В этом задании вы построите упрощённый вариант модели CLAP (Contrastive Language-Audio Pretraining):\n",
        "\n",
        "- аудио прогоняется через предобученный аудио-энкодер (например, `LanguageBindAudio`, `CNN14/16` или другой);\n",
        "- текстовое описание пропускается через предобученный текстовый энкодер CLIP;\n",
        "- поверх аудио-векторов обучается линейный адаптер, который отображает аудио в то же пространство, что и текстовые эмбеддинги CLIP;\n",
        "- обучение идёт по *контрастивному лоссу*, все энкодеры заморожены, обучаются только параметры аудио-проекции (и, при желании, температура в лоссе);\n",
        "- качество полученного аудио-текстового пространства оценивается на задаче классификации / retrieval аудио по текстам на `AudioCaps`.\n",
        "\n",
        "Идея оценки: если всё сделано правильно, для аудио и его описания косинусное сходство эмбеддингов будет выше, чем для аудио и нерелевантных текстов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJupjjbsrTwJ"
      },
      "source": [
        "**Формулировка задач**\n",
        "\n",
        "0. Выбор аудио-энкодера.\n",
        "   Выберите и обоснуйте предобученный аудио-энбеддер:  \n",
        "   - `LanguageBindAudio`,  \n",
        "   - или CNN-модель (например, PANNs CNN14/16),  \n",
        "   - или другой открытый аудио-энкодер, который выдаёт фиксированный эмбеддинг.\n",
        "\n",
        "1. Подсчёт эмбеддингов.\n",
        "   - Посчитайте аудио-векторы для всех аудио из `AudioCaps` с помощью выбранного энкодера.  \n",
        "   - Посчитайте текстовые векторы для подписей с помощью `CLIP text encoder`.\n",
        "\n",
        "2. Линейная аудио-проекция.\n",
        "   - Реализуйте модель `AudioProjection`, переводящую аудио-эмбеддинг в размерность текстового эмбеддинга CLIP.\n",
        "\n",
        "3. Контрастивное обучение.\n",
        "   - Обучите аудио-проекцию на датасете `AudioCaps` по схеме аудио ↔ текст с контрастивным лоссом.  \n",
        "   - Аудио-энкодер и CLIP должны быть полностью заморожены.\n",
        "\n",
        "4. Оценка качества.\n",
        "   - Оцените качество полученного аудио-текстового пространства на задаче классификации/ретривала аудио:  \n",
        "     для каждого аудио найдите наиболее похожую текстовую подпись в батче/валидации и посчитайте `accuracy@1/3/10`.  \n",
        "   - Сравните результаты с *случайным бейзлайном*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jAx63StYdA"
      },
      "source": [
        "### Сеттинг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw2WU446ta5Y"
      },
      "source": [
        "> Подготовьте все необходимые импорты и загрузите необходимые данные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchaudio\n",
        "import soundfile as sf  # для загрузки .flac файлов\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model  # для wav2vec2\n",
        "import clip  # для CLIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !kaggle datasets download nickkar30/audiocaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SbHqYh_Qt4BL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in DATA_ROOT: ['audiocaps_test_new.tsv', 'val_texts.json', 'test_texts.json', 'audiocaps_train.tsv', 'audio', 'audiocaps_test.tsv', 'audiocaps_val_new.tsv', 'audiocaps_val.tsv']\n"
          ]
        }
      ],
      "source": [
        "# Для загрузки AudioCaps можно воспользоваться этим кодом\n",
        "\n",
        "# !gdown --id 1FAVKNWXp5afgoNmclDwnj8j_OFTBRmIb -O audiocaps.zip\n",
        "# !gdown --id 1fWZ0DN6IbSdjM_N0zTKyDQbypP053Y5l -O audiocaps.zip\n",
        "# !unzip -q audiocaps.zip -d audiocaps\n",
        "\n",
        "DATA_ROOT = \"./audiocaps/audiocaps\"\n",
        "print(\"Files in DATA_ROOT:\", os.listdir(DATA_ROOT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCcgZWsbthgL"
      },
      "source": [
        "### Задание 1. Подготовка аудио- и текстовых энкодеров (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RbbFApuuXml"
      },
      "source": [
        "В этом задании вам нужно:\n",
        "\n",
        "1. Выбрать аудио-энкодер и инициализировать его.\n",
        "2. Инициализировать текстовый энкодер CLIP. Вы свободны выбирать самостоятельно, какой имеено.\n",
        "3. Заморозить параметры обоих энкодеров (мы не дообучаем их, а учим только линейный адаптер).\n",
        "\n",
        "Вы можете:\n",
        "\n",
        "* использовать `LanguageBindAudio` (потребует установки репозитория и зависимостей);\n",
        "* или подставить свою аудио-модель (главное - чтобы на выходе был вектор фиксированной размерности).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p7mM2dkqtYDI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading wav2vec2 model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yc-user/hse-fcs-multimodal-course/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio encoder loaded. Hidden size: 768\n",
            "Note: wav2vec2 outputs sequence embeddings, will use mean pooling for fixed-size embedding\n",
            "Loading CLIP model...\n",
            "CLIP model loaded. Text embedding dimension: 512\n",
            "\n",
            "==================================================\n",
            "Models initialized successfully!\n",
            "Audio encoder (wav2vec2) hidden size: 768\n",
            "  -> After mean pooling: 768 (fixed-size embedding)\n",
            "CLIP text embedding dim: 512\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# ┌(ಠ_ಠ)┘\n",
        "# Инициализация устройств\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Инициализация аудио-энкодера wav2vec2\n",
        "print(\"Loading wav2vec2 model...\")\n",
        "audio_model_name = \"facebook/wav2vec2-base\"  # можно использовать также \"facebook/wav2vec2-large\"\n",
        "audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_name)\n",
        "audio_model = Wav2Vec2Model.from_pretrained(audio_model_name).to(device)\n",
        "\n",
        "# Заморозка параметров аудио-энкодера\n",
        "for param in audio_model.parameters():\n",
        "    param.requires_grad = False\n",
        "audio_model.eval()\n",
        "print(f\"Audio encoder loaded. Hidden size: {audio_model.config.hidden_size}\")\n",
        "print(\"Note: wav2vec2 outputs sequence embeddings, will use mean pooling for fixed-size embedding\")\n",
        "\n",
        "# 2. Инициализация текстового энкодера CLIP\n",
        "print(\"Loading CLIP model...\")\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Заморозка параметров CLIP\n",
        "for param in clip_model.parameters():\n",
        "    param.requires_grad = False\n",
        "clip_model.eval()\n",
        "\n",
        "# Проверка размерности текстовых эмбеддингов CLIP\n",
        "test_text = clip.tokenize([\"test\"]).to(device)\n",
        "with torch.no_grad():\n",
        "    test_text_embedding = clip_model.encode_text(test_text)\n",
        "clip_text_dim = test_text_embedding.shape[-1]\n",
        "print(f\"CLIP model loaded. Text embedding dimension: {clip_text_dim}\")\n",
        "\n",
        "# Проверка размерностей\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Models initialized successfully!\")\n",
        "print(f\"Audio encoder (wav2vec2) hidden size: {audio_model.config.hidden_size}\")\n",
        "print(f\"  -> After mean pooling: {audio_model.config.hidden_size} (fixed-size embedding)\")\n",
        "print(f\"CLIP text embedding dim: {clip_text_dim}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6BdjCPcvrK7"
      },
      "source": [
        "### Задание 2. Предподсчёт аудио- и текстовых эмбеддингов (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5kz9a_VvxCs"
      },
      "source": [
        "> Важный момент, который пригодится вам и в других домашних.\n",
        "\n",
        "Чтобы не тратить время на многократный прогон энкодеров при обучении, следует:\n",
        "\n",
        "1. Предварительно посчитывать аудио-эмбеддинги для каждого `.flac` в train/val/test.\n",
        "2. Записывать их в файл формата `pickle` (например), где ключ - имя файла, значение - numpy-вектор.\n",
        "3. Аналогично посчитать текстовые эмбеддинги для подписей через CLIP и совместить их с аудио.\n",
        "\n",
        "Рекомендуемая структура:\n",
        "\n",
        "* функция `extract_audio_vectors_with_checkpointing(...)` - обходит файлы, считает эмбеддинги, периодически делает чекпоинты;\n",
        "* функция `extract_text_embeddings(texts, clip_model, clip_processor)` - возвращает список текстовых эмбеддингов;\n",
        "* функция `process_dataset(...)` - читает `.tsv`, мержит аудио-эмбеддинги и текстовые, сохраняет список словарей вида  \n",
        "  `{\"uniq_id\": ..., \"audio_embedding\": ..., \"text_embedding\": ...}` в pickle.\n",
        "\n",
        "> Вы вольны отходить от предлагаемой структуры."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S9OIM6ojkKQk"
      },
      "outputs": [],
      "source": [
        "# your code here (づ｡◕‿‿◕｡)づ\n",
        "# Базовые функции для обработки аудио и текста\n",
        "\n",
        "def load_audio_file(audio_path, target_sr=16000):\n",
        "    \"\"\"\n",
        "    Загружает аудио файл и ресемплирует до целевой частоты дискретизации.\n",
        "    \n",
        "    Args:\n",
        "        audio_path: путь к аудио файлу (.flac)\n",
        "        target_sr: целевая частота дискретизации (по умолчанию 16kHz для wav2vec2)\n",
        "    \n",
        "    Returns:\n",
        "        waveform: numpy array с аудио данными, shape (n_samples,)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Загружаем аудио файл через soundfile (работает с .flac без torchcodec)\n",
        "        waveform, sample_rate = sf.read(audio_path, dtype='float32')\n",
        "        \n",
        "        # Конвертируем в моно (если стерео)\n",
        "        if len(waveform.shape) > 1:\n",
        "            waveform = np.mean(waveform, axis=1)\n",
        "        \n",
        "        # Ресемплинг до целевой частоты, если нужно\n",
        "        if sample_rate != target_sr:\n",
        "            # Используем torchaudio для ресемплинга (более точный)\n",
        "            waveform_tensor = torch.from_numpy(waveform).unsqueeze(0)\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
        "            waveform_tensor = resampler(waveform_tensor)\n",
        "            waveform = waveform_tensor.squeeze(0).numpy()\n",
        "        \n",
        "        return waveform\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_audio_embedding(audio_path, audio_processor, audio_model, device, data_root=None):\n",
        "    \"\"\"\n",
        "    Извлекает фиксированный эмбеддинг из аудио файла с помощью wav2vec2.\n",
        "    \n",
        "    Args:\n",
        "        audio_path: путь к аудио файлу (может быть относительным или абсолютным)\n",
        "                   В TSV файлах путь имеет вид \"audiocaps/audio/val/0.flac\"\n",
        "        audio_processor: Wav2Vec2Processor\n",
        "        audio_model: Wav2Vec2Model (должна быть в eval режиме и на device)\n",
        "        device: устройство (cuda/cpu)\n",
        "        data_root: корневая директория (например, \"./audiocaps/audiocaps\")\n",
        "    \n",
        "    Returns:\n",
        "        embedding: numpy array размерности [hidden_size] (768 для wav2vec2-base)\n",
        "                   или None в случае ошибки\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Формируем полный путь\n",
        "        if data_root:\n",
        "            # Если путь уже содержит \"audiocaps/\", убираем его\n",
        "            if audio_path.startswith(\"audiocaps/\"):\n",
        "                audio_path = audio_path[len(\"audiocaps/\"):]\n",
        "            full_path = Path(data_root) / audio_path\n",
        "        else:\n",
        "            full_path = Path(audio_path)\n",
        "        \n",
        "        # Проверяем существование файла\n",
        "        if not full_path.exists():\n",
        "            print(f\"Audio file not found: {full_path}\")\n",
        "            return None\n",
        "        \n",
        "        # Загружаем аудио (Path можно использовать как строка для soundfile)\n",
        "        waveform = load_audio_file(str(full_path))\n",
        "        if waveform is None:\n",
        "            return None\n",
        "        \n",
        "        # Обрабатываем через процессор wav2vec2\n",
        "        inputs = audio_processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs.input_values.to(device)\n",
        "        \n",
        "        # Извлекаем эмбеддинги\n",
        "        with torch.no_grad():\n",
        "            outputs = audio_model(input_values)\n",
        "            # Mean pooling по временной оси для получения фиксированного эмбеддинга\n",
        "            # outputs.last_hidden_state shape: [batch, seq_len, hidden_size]\n",
        "            audio_embedding = outputs.last_hidden_state.mean(dim=1)  # [batch, hidden_size]\n",
        "            audio_embedding = audio_embedding.squeeze(0).cpu().numpy()  # [hidden_size]\n",
        "        \n",
        "        return audio_embedding\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting embedding from {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_embeddings(texts, clip_model, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    Извлекает текстовые эмбеддинги для списка текстов с помощью CLIP.\n",
        "    \n",
        "    Args:\n",
        "        texts: список строк с текстами\n",
        "        clip_model: CLIP модель (должна быть в eval режиме и на device)\n",
        "        device: устройство (cuda/cpu)\n",
        "        batch_size: размер батча для обработки\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: список numpy массивов, каждый размерности [text_dim] (512 для CLIP ViT-B/32)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    \n",
        "    # Обрабатываем тексты батчами\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting text embeddings\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        \n",
        "        # Токенизация\n",
        "        text_tokens = clip.tokenize(batch_texts).to(device)\n",
        "        \n",
        "        # Извлечение эмбеддингов\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = clip_model.encode_text(text_tokens)\n",
        "            text_embeddings = text_embeddings.cpu().numpy()\n",
        "        \n",
        "        all_embeddings.extend(text_embeddings)\n",
        "    \n",
        "    return all_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing basic functions...\n",
            "\n",
            "1. Testing load_audio_file on audiocaps/audiocaps/audio/val/0.flac\n",
            "   Audio loaded: shape=(160000,), dtype=float32\n",
            "\n",
            "2. Testing extract_audio_embedding with TSV-style path: audiocaps/audio/val/0.flac\n",
            "   Audio embedding extracted: shape=(768,), dtype=float32\n",
            "\n",
            "3. Testing extract_text_embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting text embeddings: 100%|██████████| 1/1 [00:00<00:00, 96.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text embeddings extracted: 2 embeddings, shape=(512,)\n",
            "\n",
            "Basic functions are ready!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Тестирование базовых функций на одном примере\n",
        "print(\"Testing basic functions...\")\n",
        "# Используем путь как в TSV файле: \"audiocaps/audio/val/0.flac\"\n",
        "test_audio_path_tsv = \"audiocaps/audio/val/0.flac\"\n",
        "# Или прямой путь для теста load_audio_file\n",
        "test_audio_path_direct = Path(DATA_ROOT) / \"audio/val/0.flac\"\n",
        "\n",
        "if test_audio_path_direct.exists():\n",
        "    print(f\"\\n1. Testing load_audio_file on {test_audio_path_direct}\")\n",
        "    test_waveform = load_audio_file(test_audio_path_direct)\n",
        "    if test_waveform is not None:\n",
        "        print(f\"   Audio loaded: shape={test_waveform.shape}, dtype={test_waveform.dtype}\")\n",
        "    \n",
        "    print(f\"\\n2. Testing extract_audio_embedding with TSV-style path: {test_audio_path_tsv}\")\n",
        "    test_audio_emb = extract_audio_embedding(test_audio_path_tsv, audio_processor, audio_model, device, DATA_ROOT)\n",
        "    if test_audio_emb is not None:\n",
        "        print(f\"   Audio embedding extracted: shape={test_audio_emb.shape}, dtype={test_audio_emb.dtype}\")\n",
        "else:\n",
        "    print(f\"  [WARN] Test audio file not found: {test_audio_path_direct}\")\n",
        "\n",
        "print(\"\\n3. Testing extract_text_embeddings\")\n",
        "test_texts = [\"A woman talks nearby as water pours\", \"Multiple clanging and clanking sounds\"]\n",
        "test_text_embs = extract_text_embeddings(test_texts, clip_model, device, batch_size=2)\n",
        "if test_text_embs:\n",
        "    print(f\"Text embeddings extracted: {len(test_text_embs)} embeddings, shape={test_text_embs[0].shape}\")\n",
        "\n",
        "print(\"\\nBasic functions are ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Дополнительные функции для обработки аудио и текста\n",
        "\n",
        "def extract_audio_vectors_with_checkpointing(\n",
        "    tsv_path, \n",
        "    audio_processor, \n",
        "    audio_model, \n",
        "    device, \n",
        "    data_root,\n",
        "    checkpoint_dir=\"./checkpoints\",\n",
        "    checkpoint_interval=1000,\n",
        "    split_name=\"\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Извлекает аудио-эмбеддинги для всех аудио из TSV файла с поддержкой чекпоинтинга.\n",
        "    \n",
        "    Args:\n",
        "        tsv_path: путь к TSV файлу с данными\n",
        "        audio_processor: Wav2Vec2Processor\n",
        "        audio_model: Wav2Vec2Model\n",
        "        device: устройство (cuda/cpu)\n",
        "        data_root: корневая директория для аудио файлов\n",
        "        checkpoint_dir: директория для сохранения чекпоинтов\n",
        "        checkpoint_interval: интервал сохранения чекпоинтов (количество файлов)\n",
        "        split_name: имя сплита (train/val/test) для имени файла чекпоинта\n",
        "    \n",
        "    Returns:\n",
        "        audio_embeddings_dict: словарь {uniq_id: numpy_array}\n",
        "    \"\"\"\n",
        "    # Создаём директорию для чекпоинтов\n",
        "    Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Имя файла чекпоинта\n",
        "    checkpoint_file = str(Path(checkpoint_dir) / f\"audio_embeddings_{split_name}.pkl\")\n",
        "    \n",
        "    # Загружаем существующий чекпоинт, если есть\n",
        "    audio_embeddings_dict = {}\n",
        "    if Path(checkpoint_file).exists():\n",
        "        print(f\"Loading checkpoint from {checkpoint_file}...\")\n",
        "        with open(checkpoint_file, 'rb') as f:\n",
        "            audio_embeddings_dict = pickle.load(f)\n",
        "        print(f\"Loaded {len(audio_embeddings_dict)} embeddings from checkpoint\")\n",
        "    \n",
        "    # Читаем TSV файл\n",
        "    print(f\"Reading TSV file: {tsv_path}\")\n",
        "    df = pd.read_csv(tsv_path, sep='\\t')\n",
        "    print(f\"Total rows in TSV: {len(df)}\")\n",
        "    \n",
        "    # Обрабатываем каждый аудио файл\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "    error_count = 0\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Extracting audio embeddings ({split_name})\"):\n",
        "        uniq_id = row['uniq_id']\n",
        "        audio_path = row['audio']\n",
        "        \n",
        "        # Пропускаем, если уже обработано\n",
        "        if uniq_id in audio_embeddings_dict:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        \n",
        "        # Извлекаем эмбеддинг\n",
        "        audio_embedding = extract_audio_embedding(\n",
        "            audio_path, \n",
        "            audio_processor, \n",
        "            audio_model, \n",
        "            device, \n",
        "            data_root\n",
        "        )\n",
        "        \n",
        "        if audio_embedding is not None:\n",
        "            audio_embeddings_dict[uniq_id] = audio_embedding\n",
        "            processed_count += 1\n",
        "        else:\n",
        "            error_count += 1\n",
        "            print(f\" [WARN] Failed to extract embedding for uniq_id={uniq_id}, audio={audio_path}\")\n",
        "        \n",
        "        # Сохраняем чекпоинт периодически\n",
        "        if processed_count > 0 and processed_count % checkpoint_interval == 0:\n",
        "            print(f\"Saving checkpoint after {processed_count} processed files...\")\n",
        "            with open(checkpoint_file, 'wb') as f:\n",
        "                pickle.dump(audio_embeddings_dict, f)\n",
        "    \n",
        "    # Финальное сохранение\n",
        "    print(\"Saving final checkpoint...\")\n",
        "    with open(checkpoint_file, 'wb') as f:\n",
        "        pickle.dump(audio_embeddings_dict, f)\n",
        "    \n",
        "    print(\"\\n[OK] Audio embeddings extraction completed!\")\n",
        "    print(f\"   - Processed: {processed_count}\")\n",
        "    print(f\"   - Skipped (already in checkpoint): {skipped_count}\")\n",
        "    print(f\"   - Errors: {error_count}\")\n",
        "    print(f\"   - Total embeddings: {len(audio_embeddings_dict)}\")\n",
        "    \n",
        "    return audio_embeddings_dict\n",
        "\n",
        "\n",
        "def process_dataset(\n",
        "    tsv_path,\n",
        "    audio_processor,\n",
        "    audio_model,\n",
        "    clip_model,\n",
        "    device,\n",
        "    data_root,\n",
        "    checkpoint_dir=\"./checkpoints\",\n",
        "    split_name=\"\",\n",
        "    checkpoint_interval=1000\n",
        "):\n",
        "    \"\"\"\n",
        "    Обрабатывает датасет: извлекает аудио- и текстовые эмбеддинги и сохраняет в pickle.\n",
        "    \n",
        "    Args:\n",
        "        tsv_path: путь к TSV файлу\n",
        "        audio_processor: Wav2Vec2 Processor\n",
        "        audio_model: Wav2Vec2 Model\n",
        "        clip_model: CLIP модель\n",
        "        device: устройство (cuda/cpu)\n",
        "        data_root: корневая директория для аудио файлов\n",
        "        checkpoint_dir: директория для чекпоинтов\n",
        "        split_name: имя сплита (train/val/test)\n",
        "        checkpoint_interval: интервал сохранения чекпоинтов\n",
        "    \n",
        "    Returns:\n",
        "        dataset: список словарей вида {\"uniq_id\": ..., \"audio_embedding\": ..., \"text_embedding\": ..., ...}\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing dataset: {split_name}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # 1. Извлекаем аудио-эмбеддинги\n",
        "    print(\"Step 1: Extracting audio embeddings...\")\n",
        "    audio_embeddings_dict = extract_audio_vectors_with_checkpointing(\n",
        "        tsv_path,\n",
        "        audio_processor,\n",
        "        audio_model,\n",
        "        device,\n",
        "        data_root,\n",
        "        checkpoint_dir,\n",
        "        checkpoint_interval,\n",
        "        split_name\n",
        "    )\n",
        "    \n",
        "    # 2. Читаем TSV для получения текстов\n",
        "    print(\"\\nStep 2: Reading TSV file for texts...\")\n",
        "    df = pd.read_csv(tsv_path, sep='\\t')\n",
        "    texts = df['text'].tolist()\n",
        "    uniq_ids = df['uniq_id'].tolist()\n",
        "    audio_paths = df['audio'].tolist()\n",
        "    \n",
        "    # 3. Извлекаем текстовые эмбеддинги\n",
        "    print(\"\\nStep 3: Extracting text embeddings...\")\n",
        "    text_embeddings = extract_text_embeddings(texts, clip_model, device, batch_size=32)\n",
        "    \n",
        "    # 4. Объединяем данные\n",
        "    print(\"\\nStep 4: Combining audio and text embeddings...\")\n",
        "    dataset = []\n",
        "    missing_audio_count = 0\n",
        "    \n",
        "    for i, uniq_id in enumerate(uniq_ids):\n",
        "        if uniq_id in audio_embeddings_dict:\n",
        "            dataset.append({\n",
        "                \"uniq_id\": uniq_id,\n",
        "                \"audio_embedding\": audio_embeddings_dict[uniq_id],\n",
        "                \"text_embedding\": text_embeddings[i],\n",
        "                \"text\": texts[i],\n",
        "                \"audio_path\": audio_paths[i]\n",
        "            })\n",
        "        else:\n",
        "            missing_audio_count += 1\n",
        "            print(f\" [WARN] Missing audio embedding for uniq_id={uniq_id}\")\n",
        "    \n",
        "    # 5. Сохраняем финальный датасет\n",
        "    output_file = os.path.join(checkpoint_dir, f\"embeddings_{split_name}.pkl\")\n",
        "    print(f\"\\nStep 5: Saving combined dataset to {output_file}...\")\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "    \n",
        "    print(\"\\n[OK] Dataset processing completed!\")\n",
        "    print(f\"   - Total samples: {len(dataset)}\")\n",
        "    print(f\"   - Missing audio embeddings: {missing_audio_count}\")\n",
        "    print(f\"   - Saved to: {output_file}\")\n",
        "    \n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Processing dataset: val\n",
            "============================================================\n",
            "\n",
            "Step 1: Extracting audio embeddings...\n",
            "Loading checkpoint from checkpoints/audio_embeddings_val.pkl...\n",
            "Loaded 495 embeddings from checkpoint\n",
            "Reading TSV file: audiocaps/audiocaps/audiocaps_val_new.tsv\n",
            "Total rows in TSV: 495\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting audio embeddings (val): 100%|██████████| 495/495 [00:00<00:00, 20552.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final checkpoint...\n",
            "\n",
            "[OK] Audio embeddings extraction completed!\n",
            "   - Processed: 0\n",
            "   - Skipped (already in checkpoint): 495\n",
            "   - Errors: 0\n",
            "   - Total embeddings: 495\n",
            "\n",
            "Step 2: Reading TSV file for texts...\n",
            "\n",
            "Step 3: Extracting text embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting text embeddings: 100%|██████████| 16/16 [00:00<00:00, 78.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4: Combining audio and text embeddings...\n",
            "\n",
            "Step 5: Saving combined dataset to ./checkpoints/embeddings_val.pkl...\n",
            "\n",
            "[OK] Dataset processing completed!\n",
            "   - Total samples: 495\n",
            "   - Missing audio embeddings: 0\n",
            "   - Saved to: ./checkpoints/embeddings_val.pkl\n",
            "\n",
            "============================================================\n",
            "Processing dataset: test\n",
            "============================================================\n",
            "\n",
            "Step 1: Extracting audio embeddings...\n",
            "Loading checkpoint from checkpoints/audio_embeddings_test.pkl...\n",
            "Loaded 963 embeddings from checkpoint\n",
            "Reading TSV file: audiocaps/audiocaps/audiocaps_test_new.tsv\n",
            "Total rows in TSV: 963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting audio embeddings (test): 100%|██████████| 963/963 [00:00<00:00, 22604.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final checkpoint...\n",
            "\n",
            "[OK] Audio embeddings extraction completed!\n",
            "   - Processed: 0\n",
            "   - Skipped (already in checkpoint): 963\n",
            "   - Errors: 0\n",
            "   - Total embeddings: 963\n",
            "\n",
            "Step 2: Reading TSV file for texts...\n",
            "\n",
            "Step 3: Extracting text embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting text embeddings: 100%|██████████| 31/31 [00:00<00:00, 87.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4: Combining audio and text embeddings...\n",
            "\n",
            "Step 5: Saving combined dataset to ./checkpoints/embeddings_test.pkl...\n",
            "\n",
            "[OK] Dataset processing completed!\n",
            "   - Total samples: 963\n",
            "   - Missing audio embeddings: 0\n",
            "   - Saved to: ./checkpoints/embeddings_test.pkl\n",
            "\n",
            "============================================================\n",
            "Processing dataset: train\n",
            "============================================================\n",
            "\n",
            "Step 1: Extracting audio embeddings...\n",
            "Loading checkpoint from checkpoints/audio_embeddings_train.pkl...\n",
            "Loaded 49490 embeddings from checkpoint\n",
            "Reading TSV file: audiocaps/audiocaps/audiocaps_train.tsv\n",
            "Total rows in TSV: 49490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting audio embeddings (train): 100%|██████████| 49490/49490 [00:02<00:00, 23356.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final checkpoint...\n",
            "\n",
            "[OK] Audio embeddings extraction completed!\n",
            "   - Processed: 0\n",
            "   - Skipped (already in checkpoint): 49490\n",
            "   - Errors: 0\n",
            "   - Total embeddings: 49490\n",
            "\n",
            "Step 2: Reading TSV file for texts...\n",
            "\n",
            "Step 3: Extracting text embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting text embeddings: 100%|██████████| 1547/1547 [00:16<00:00, 91.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4: Combining audio and text embeddings...\n",
            "\n",
            "Step 5: Saving combined dataset to ./checkpoints/embeddings_train.pkl...\n",
            "\n",
            "[OK] Dataset processing completed!\n",
            "   - Total samples: 49490\n",
            "   - Missing audio embeddings: 0\n",
            "   - Saved to: ./checkpoints/embeddings_train.pkl\n"
          ]
        }
      ],
      "source": [
        "# Обработка датасетов\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "\n",
        "val_tsv = str(Path(DATA_ROOT) / \"audiocaps_val_new.tsv\")\n",
        "val_dataset = process_dataset(\n",
        "    val_tsv,\n",
        "    audio_processor,\n",
        "    audio_model,\n",
        "    clip_model,\n",
        "    device,\n",
        "    DATA_ROOT,\n",
        "    checkpoint_dir,\n",
        "    split_name=\"val\",\n",
        "    checkpoint_interval=100\n",
        ")\n",
        "\n",
        "test_tsv = str(Path(DATA_ROOT) / \"audiocaps_test_new.tsv\")\n",
        "test_dataset = process_dataset(\n",
        "    test_tsv,\n",
        "    audio_processor,\n",
        "    audio_model,\n",
        "    clip_model,\n",
        "    device,\n",
        "    DATA_ROOT,\n",
        "    checkpoint_dir,\n",
        "    split_name=\"test\",\n",
        "    checkpoint_interval=100\n",
        ")\n",
        "\n",
        "train_tsv = str(Path(DATA_ROOT) / \"audiocaps_train.tsv\")\n",
        "train_dataset = process_dataset(\n",
        "    train_tsv,\n",
        "    audio_processor,\n",
        "    audio_model,\n",
        "    clip_model,\n",
        "    device,\n",
        "    DATA_ROOT,\n",
        "    checkpoint_dir,\n",
        "    split_name=\"train\",\n",
        "    checkpoint_interval=1000\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OD_skxfxBVY"
      },
      "source": [
        "### Задание 3. Линейный аудио-адаптер и контрастивный лосс (3 балла)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCDte8VBw9ql"
      },
      "source": [
        "Теперь, когда у нас есть пары *audio_embedding, text_embedding*, реализуем:\n",
        "\n",
        "1. Класс `AudioTextDataset`, который читает pickle с комбинированными эмбеддингами.\n",
        "2. Линейную модель `AudioProjection`, переводящую аудио-эмбеддинг в размерность текстового.\n",
        "3. Контрастивный лосс для аудио↔текст:\n",
        "   - нормализовать эмбеддинги по L2;\n",
        "   - посчитать матрицу сходства;\n",
        "   - задать таргеты как `targets = arange(batch_size)`;\n",
        "   - вычислить `CrossEntropyLoss` как для строк audio→text и для строк text→audio, усреднить.\n",
        "\n",
        "Обучаем **только** `AudioProjection` (и, по желанию, параметр temperature).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypQ42sQ9zs7Q"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# (╯°□°）╯︵ ┻━┻"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-kkEEuzx_Vi"
      },
      "source": [
        "### Задание 4. Оценка качества на задаче классификации аудио (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQbXbIAyRM-"
      },
      "source": [
        "\n",
        "Теперь нужно понять, насколько хорошо аудио-векторы после проекции \"попадают\" в пространство текстовых эмбеддингов:\n",
        "\n",
        "1. Посчитайте проекции аудио для всех примеров в валидации.\n",
        "2. Для каждого аудио найдите `top-k` наиболее похожих текстов по косинусному сходству (или скалярному произведению после L2-нормализации).\n",
        "3. Посчитайте `accuracy@1`, `accuracy@3`, `accuracy@10`, т.е. долю случаев, когда \"правильный\" текст попал в топ-k.\n",
        "4. Сравните с неким *случайным бейзлайном*: для каждого аудио выберите `k` случайных текстов и посчитайте такую же метрику.\n",
        "\n",
        "> Важно: в батче класс \"правильного\" текста для i-го аудио - это индекс i (как в контрастивном лоссе)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFrO9tRoylVl"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# (⌐■_■)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEoUVkEQxp05"
      },
      "source": [
        "### Вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1e1D5QMy3yP"
      },
      "source": [
        "Оформите, пожалуйста, небольшой вывод. Например, можно воспрользоваться следующим планом:\n",
        "\n",
        "   * какую аудио-модель вы выбрали и почему;\n",
        "   * как вели себя потери на обучении;\n",
        "   * какие значения метрик получились и насколько они превосходят случайный baseline;\n",
        "   * любые наблюдения (например, зависимость от числа эпох, размера батча и т.д.);\n",
        "   * милые пожелания ассистенту/лектору, который будет это проверять."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_e9rsiIzhHE"
      },
      "source": [
        "your text here (ಠ.ಠ)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
