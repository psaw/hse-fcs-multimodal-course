{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0cff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  694M  100  694M    0     0   101M      0  0:00:06  0:00:06 --:--:--  101M\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 0. Скачивание данных и чекпоинта\n",
    "# ============================================================================\n",
    "\n",
    "# !curl https://storage.yandexcloud.net/hse-ai24-mm-hw3-data/course_vqa_dataset.zip -o data/course_vqa_dataset.zip\n",
    "!curl https://storage.yandexcloud.net/hse-ai24-mm-hw3-data/vqa-clip-bert-pred-epoch=66-val_acc=0.649.ckpt -o data/VQAClipBERTpred.ckpt\n",
    "# !unzip -nq data/course_vqa_dataset.zip -d data/course_vqa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955ff40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ВОСПРОИЗВОДИМЫЙ ПАЙПЛАЙН: Формирование финального submission.csv\n",
      "================================================================================\n",
      "\n",
      "1. Загрузка данных...\n",
      "   Train: 19873 примеров\n",
      "   Test: 4969 примеров\n",
      "\n",
      "2. Подготовка данных...\n",
      "   Размер словаря ответов: 1274\n",
      "   Категорий ответов: 6\n",
      "   Типов вопросов: 12\n",
      "\n",
      "3. Загрузка модели...\n",
      "   Путь к модели: data/course_vqa_dataset/../VQAClipBERTpred.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a9ba974e-af8f-4f19-a0eb-284f4fe191b2)')' thrown while requesting HEAD https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Инференс и формирование submission.csv...\n",
      "Размер submission: 4969\n",
      "Колонки: ['ID', 'answer']\n",
      "\n",
      "Первые несколько строк:\n",
      "                 ID                 answer\n",
      "0  001506824e76191d              no_answer\n",
      "1  002677a6d6bac1dd              no_answer\n",
      "2  0027a003a50877a5  microsoft corporation\n",
      "3  002bf579cd0bfaed              no_answer\n",
      "4  0035fa712c369df8              no_answer\n",
      "5  0042dd305b63ce2d                  money\n",
      "6  0043d79066103187              no_answer\n",
      "7  004741a80abd988c               keyboard\n",
      "8  004c59b16f26a4f7              no_answer\n",
      "9  005aa83ea31a8235              no_answer\n",
      "\n",
      "Submission сохранён в data/course_vqa_dataset/submission.csv\n",
      "Проверка формата:\n",
      "  Размер sample_submission: 4969\n",
      "  Размер нашего submission: 4969\n",
      "  Совпадают ID: True\n",
      "\n",
      "================================================================================\n",
      "ПАЙПЛАЙН ЗАВЕРШЁН!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Воспроизводимый пайплайн: формирование финального submission.csv\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ИМПОРТЫ\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import re\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torchmetrics import Accuracy\n",
    "from torch.nn import MultiheadAttention\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ============================================================================\n",
    "# 2. КОНСТАНТЫ И ПУТИ\n",
    "# ============================================================================\n",
    "DATA_DIR = Path(\"./data/course_vqa_dataset\")\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "TEST_PATH = DATA_DIR / \"test.csv\"\n",
    "SAMPLE_SUB_PATH = DATA_DIR / \"sample_submission.csv\"\n",
    "MODEL_PATH = DATA_DIR / \"../VQAClipBERTpred.ckpt\"\n",
    "SUBMISSION_PATH = DATA_DIR / \"submission.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
    "# ============================================================================\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def tokenize(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", text)\n",
    "    tokens = text.strip().split()\n",
    "    return tokens\n",
    "\n",
    "def build_word_vocab(texts, min_freq: int = 1, max_size: int | None = None):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(tokenize(t))\n",
    "    \n",
    "    words_and_counts = [(w, c) for w, c in counter.items() if c >= min_freq]\n",
    "    words_and_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    if max_size is not None:\n",
    "        words_and_counts = words_and_counts[:max_size]\n",
    "    \n",
    "    vocab_words = [w for w, _ in words_and_counts]\n",
    "    word2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for w in vocab_words:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = len(word2id)\n",
    "    \n",
    "    id2word = {i: w for w, i in word2id.items()}\n",
    "    return word2id, id2word\n",
    "\n",
    "def encode_question(text: str, word2id: dict, max_len: int = 20):\n",
    "    tokens = tokenize(text)\n",
    "    ids = []\n",
    "    for tok in tokens[:max_len]:\n",
    "        ids.append(word2id.get(tok, word2id[UNK_TOKEN]))\n",
    "    if len(ids) < max_len:\n",
    "        ids += [word2id[PAD_TOKEN]] * (max_len - len(ids))\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def build_answer_vocab(train_df, min_freq: int = 1, max_word_count: int = 0):\n",
    "    counts = Counter(train_df[\"answer\"])\n",
    "    \n",
    "    if min_freq > 1:\n",
    "        rare_answers = {ans for ans, c in counts.items() if c < min_freq}\n",
    "        train_df = train_df.copy()\n",
    "        train_df[\"answer\"] = train_df[\"answer\"].apply(\n",
    "            lambda x: \"no_answer\" if x in rare_answers else x\n",
    "        )\n",
    "        counts = Counter(train_df[\"answer\"])\n",
    "    \n",
    "    if max_word_count > 0:\n",
    "        train_df = train_df.copy()\n",
    "        train_df[\"answer\"] = train_df[\"answer\"].apply(\n",
    "            lambda x: \"no_answer\" if len(x.split()) > max_word_count else x\n",
    "        )\n",
    "        counts = Counter(train_df[\"answer\"])\n",
    "    \n",
    "    answers = sorted(counts.keys())\n",
    "    answer2id = {ans: idx for idx, ans in enumerate(answers)}\n",
    "    id2answer = {idx: ans for ans, idx in answer2id.items()}\n",
    "    \n",
    "    return answer2id, id2answer, train_df\n",
    "\n",
    "def classify_question_type(question: str) -> str:\n",
    "    question_lower = question.lower().strip()\n",
    "    \n",
    "    if question_lower.startswith('what'):\n",
    "        if 'color' in question_lower or 'colour' in question_lower:\n",
    "            return 'what_color'\n",
    "        elif 'text' in question_lower or 'say' in question_lower or 'read' in question_lower:\n",
    "            return 'what_text'\n",
    "        elif 'number' in question_lower or 'count' in question_lower or 'many' in question_lower:\n",
    "            return 'what_number'\n",
    "        else:\n",
    "            return 'what_other'\n",
    "    elif question_lower.startswith('where'):\n",
    "        return 'where'\n",
    "    elif question_lower.startswith('when'):\n",
    "        return 'when'\n",
    "    elif question_lower.startswith('who'):\n",
    "        return 'who'\n",
    "    elif question_lower.startswith('why'):\n",
    "        return 'why'\n",
    "    elif question_lower.startswith('how'):\n",
    "        if 'many' in question_lower or 'much' in question_lower:\n",
    "            return 'how_many'\n",
    "        else:\n",
    "            return 'how_other'\n",
    "    elif question_lower.startswith(('is ', 'are ', 'was ', 'were ', 'do ', 'does ', 'did ', 'can ', 'could ', 'will ', 'would ')):\n",
    "        return 'yes_no'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def categorize_answer(answer: str) -> str:\n",
    "    if answer == 'no_answer':\n",
    "        return 'no_answer'\n",
    "    \n",
    "    answer_lower = answer.lower().strip()\n",
    "    colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', \n",
    "              'black', 'white', 'grey', 'gray', 'brown', 'tan', 'beige']\n",
    "    if answer_lower in colors:\n",
    "        return 'color'\n",
    "    \n",
    "    if answer_lower.isdigit() or (answer_lower.replace('.', '').isdigit()):\n",
    "        return 'number'\n",
    "    \n",
    "    if answer_lower in ['yes', 'no']:\n",
    "        return 'yes_no'\n",
    "    \n",
    "    if len(answer.split()) == 1:\n",
    "        return 'single_word'\n",
    "    \n",
    "    if len(answer.split()) >= 2:\n",
    "        return 'phrase'\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. КЛАССЫ ДЛЯ ДАТАСЕТА И МОДЕЛИ\n",
    "# ============================================================================\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        images_dir,\n",
    "        word2id,\n",
    "        answer2id=None,\n",
    "        max_question_len: int = 20,\n",
    "        is_train: bool = True,\n",
    "        image_size: int = 224,\n",
    "        use_bert: bool = False,\n",
    "        augment: bool = False,\n",
    "        image_normalization: str = \"imagenet\",\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.word2id = word2id\n",
    "        self.answer2id = answer2id\n",
    "        self.max_question_len = max_question_len\n",
    "        self.is_train = is_train\n",
    "        self.use_bert = use_bert\n",
    "        self.augment = augment\n",
    "\n",
    "        if image_normalization == \"clip\":\n",
    "            mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "            std = (0.26862954, 0.26130258, 0.27577711)\n",
    "        else:\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "\n",
    "        if self.augment:\n",
    "            img_tfms = [\n",
    "                T.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            ]\n",
    "        else:\n",
    "            img_tfms = [\n",
    "                T.Resize(image_size),\n",
    "                T.CenterCrop(image_size),\n",
    "            ]\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            *img_tfms,\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.images_dir / row[\"image\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        question = row[\"question\"]\n",
    "\n",
    "        if self.use_bert:\n",
    "            question_data = question\n",
    "        else:\n",
    "            question_data = encode_question(question, self.word2id, max_len=self.max_question_len)\n",
    "\n",
    "        if self.is_train:\n",
    "            if self.use_bert:\n",
    "                assert \"answer_category\" in row and \"question_type\" in row\n",
    "                answer_category = row[\"answer_category\"]\n",
    "                question_type = row[\"question_type\"]\n",
    "            else:\n",
    "                answer_category = \"\"\n",
    "                question_type = \"\"\n",
    "            answer_str = row[\"answer\"]\n",
    "            if answer_str not in self.answer2id:\n",
    "                answer_str = \"no_answer\"\n",
    "            answer_id = self.answer2id[answer_str]\n",
    "            answer_id = torch.tensor(answer_id, dtype=torch.long)\n",
    "            return img_tensor, question_data, answer_id, answer_category, question_type\n",
    "        else:\n",
    "            qid = row[\"ID\"]\n",
    "            return img_tensor, question_data, qid, \"\", \"\"\n",
    "\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, question_dim: int, image_dim: int, hidden_dim: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(question_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(image_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(image_dim, hidden_dim)\n",
    "        self.attention = MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, question_features, image_features):\n",
    "        B = question_features.shape[0]\n",
    "        Q = self.q_proj(question_features).unsqueeze(1)\n",
    "        K = self.k_proj(image_features).unsqueeze(1)\n",
    "        V = self.v_proj(image_features).unsqueeze(1)\n",
    "        attn_out, _ = self.attention(Q, K, V)\n",
    "        attn_out = attn_out.squeeze(1)\n",
    "        q_residual = self.q_proj(question_features)\n",
    "        out = self.layer_norm(attn_out + q_residual)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "# для финального пайплайна эта модель не используется, но она базовая\n",
    "class VQAClipBERT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model_name: str = \"ViT-L/14\",\n",
    "        bert_model_name: str = \"bert-base-uncased\",\n",
    "        num_answers: int = 5524,\n",
    "        num_answer_categories: int = 10,\n",
    "        num_question_types: int = 20,\n",
    "        hidden_dim: int = 1024,\n",
    "        learning_rate: float = 5e-4,\n",
    "        freeze_image_encoder: bool = True,\n",
    "        freeze_bert: bool = True,\n",
    "        use_cross_attention: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.clip_model, self.clip_preprocess = clip.load(clip_model_name)\n",
    "        self.clip_visual = self.clip_model.visual\n",
    "\n",
    "        clip_dimensions = {\n",
    "            \"ViT-B/32\": 512,\n",
    "            \"ViT-B/16\": 512,\n",
    "            \"ViT-L/14\": 768,\n",
    "            \"RN50\": 1024,\n",
    "            \"RN101\": 512,\n",
    "            \"RN50x4\": 640,\n",
    "            \"RN50x16\": 768,\n",
    "            \"RN50x64\": 1024,\n",
    "        }\n",
    "\n",
    "        if clip_model_name in clip_dimensions:\n",
    "            image_feature_dim = clip_dimensions[clip_model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Неизвестная модель изображений: {clip_model_name}\")\n",
    "        \n",
    "        if freeze_image_encoder:\n",
    "            for param in self.clip_visual.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        question_feature_dim = self.bert_model.config.hidden_size\n",
    "        \n",
    "        # Эмбеддинги для категорий ответов и типов вопросов\n",
    "        self.answer_categories = nn.Embedding(num_answer_categories, question_feature_dim)\n",
    "        self.question_types = nn.Embedding(num_question_types, question_feature_dim)\n",
    "        \n",
    "        self.answer_category2id = None\n",
    "        self.question_type2id = None\n",
    "\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.fusion = CrossAttentionFusion(\n",
    "                question_dim=question_feature_dim,\n",
    "                image_dim=image_feature_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                num_heads=8\n",
    "            )\n",
    "            fusion_output_dim = hidden_dim\n",
    "        else:\n",
    "            fusion_input_dim = image_feature_dim + question_feature_dim\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(fusion_input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "            )\n",
    "            fusion_output_dim = hidden_dim // 2\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_output_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_answers),\n",
    "        )\n",
    "        \n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_answers)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_answers)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, images, questions, answer_categories, question_types):\n",
    "        # удалил для сокращения размера файла, т.к. этот код не используется\n",
    "        raise NotImplementedError(\"forward is not implemented\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError(\"training_step is not implemented\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError(\"validation_step is not implemented\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError(\"configure_optimizers is not implemented\")\n",
    "\n",
    "\n",
    "class VQAClipBERTpred(VQAClipBERT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model_name: str = \"ViT-L/14\",\n",
    "        bert_model_name: str = \"bert-base-uncased\",\n",
    "        num_answers: int = 5524,\n",
    "        num_answer_categories: int = 10,\n",
    "        num_question_types: int = 20,\n",
    "        hidden_dim: int = 1024,\n",
    "        learning_rate: float = 2e-5,\n",
    "        freeze_image_encoder: bool = True,\n",
    "        freeze_bert: bool = True,\n",
    "        use_cross_attention: bool = False,\n",
    "        category_loss_weight: float = 0.4,\n",
    "        type_loss_weight: float = 0.15\n",
    "    ):\n",
    "        super().__init__(\n",
    "            clip_model_name=clip_model_name,\n",
    "            bert_model_name=bert_model_name,\n",
    "            num_answers=num_answers,\n",
    "            num_answer_categories=num_answer_categories,\n",
    "            num_question_types=num_question_types,\n",
    "            hidden_dim=hidden_dim,\n",
    "            learning_rate=learning_rate,\n",
    "            freeze_image_encoder=freeze_image_encoder,\n",
    "            freeze_bert=freeze_bert,\n",
    "            use_cross_attention=use_cross_attention,\n",
    "        )\n",
    "        # веса для multi-task loss\n",
    "        self.category_loss_weight = category_loss_weight\n",
    "        self.type_loss_weight = type_loss_weight\n",
    "        \n",
    "        if use_cross_attention:\n",
    "            fusion_output_dim = hidden_dim\n",
    "        else:\n",
    "            fusion_output_dim = hidden_dim // 2  # так уж сложилось в базовой модели\n",
    "        \n",
    "        # обучаемые предикторы для категорий ответов и типов вопросов\n",
    "        self.category_predictor = nn.Sequential(\n",
    "            nn.Linear(fusion_output_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_answer_categories),\n",
    "        )\n",
    "        \n",
    "        self.type_predictor = nn.Sequential(\n",
    "            nn.Linear(fusion_output_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_question_types),\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, questions, answer_categories, question_types, predict_auxiliary=False):\n",
    "        images = images.to(dtype=self.clip_model.dtype)\n",
    "        img_features = self.clip_visual(images)\n",
    "        img_features = img_features.float()\n",
    "        \n",
    "        if isinstance(questions, tuple):\n",
    "            questions_list = list(questions)\n",
    "        elif isinstance(questions, list):\n",
    "            questions_list = questions\n",
    "        elif isinstance(questions, str):\n",
    "            questions_list = [questions]\n",
    "        else:\n",
    "            raise ValueError(f\"Неожиданный формат вопросов: {type(questions)}\")\n",
    "        \n",
    "        encoded = self.bert_tokenizer(\n",
    "            questions_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "        ).to(images.device)\n",
    "\n",
    "        with torch.set_grad_enabled(not self.hparams.freeze_bert):\n",
    "            bert_output = self.bert_model(**encoded)\n",
    "            q_features = bert_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        need_predict = predict_auxiliary or (len(answer_categories) == 0 or answer_categories[0] == \"\")\n",
    "        \n",
    "        # 2-х проходный фьюжн\n",
    "        # 1-й проход: эмбеддинг BERT + эмбеддинги меток из датасета -> cross-attention\n",
    "        # 2-й проход: добавляем эмбеддинги предсказаний предикторов -> cross-attention\n",
    "        \n",
    "        # 1-й проход\n",
    "        if not need_predict:\n",
    "            ac_list = list(answer_categories) if isinstance(answer_categories, (tuple, list)) else [answer_categories]\n",
    "            if self.answer_category2id is not None:\n",
    "                ac_indices = [self.answer_category2id.get(ac, 0) for ac in ac_list]\n",
    "            else:\n",
    "                ac_indices = [hash(ac) % self.hparams.num_answer_categories for ac in ac_list]\n",
    "            ac_indices_tensor = torch.tensor(ac_indices, dtype=torch.long, device=images.device)\n",
    "            ac_features = self.answer_categories(ac_indices_tensor)\n",
    "            \n",
    "            qt_list = list(question_types) if isinstance(question_types, (tuple, list)) else [question_types]\n",
    "            if self.question_type2id is not None:\n",
    "                qt_indices = [self.question_type2id.get(qt, 0) for qt in qt_list]\n",
    "            else:\n",
    "                qt_indices = [hash(qt) % self.hparams.num_question_types for qt in qt_list]\n",
    "            qt_indices_tensor = torch.tensor(qt_indices, dtype=torch.long, device=images.device)\n",
    "            qt_features = self.question_types(qt_indices_tensor)\n",
    "            \n",
    "            txt_features = q_features + ac_features + qt_features\n",
    "        else:\n",
    "            txt_features = q_features\n",
    "        \n",
    "        if self.use_cross_attention:\n",
    "            fused_features_initial = self.fusion(txt_features, img_features)\n",
    "        else:\n",
    "            combined_initial = torch.cat([img_features, txt_features], dim=1)\n",
    "            fused_features_initial = self.fusion(combined_initial)\n",
    "        \n",
    "        # 2-й проход\n",
    "        if need_predict:\n",
    "            category_logits = self.category_predictor(fused_features_initial)\n",
    "            type_logits = self.type_predictor(fused_features_initial)\n",
    "            \n",
    "            ac_pred_indices = category_logits.argmax(dim=1)\n",
    "            qt_pred_indices = type_logits.argmax(dim=1)\n",
    "            \n",
    "            ac_features_pred = self.answer_categories(ac_pred_indices)\n",
    "            qt_features_pred = self.question_types(qt_pred_indices)\n",
    "            \n",
    "            txt_features_final = q_features + ac_features_pred + qt_features_pred\n",
    "            \n",
    "            if self.use_cross_attention:\n",
    "                fused_features_final = self.fusion(txt_features_final, img_features)\n",
    "            else:\n",
    "                combined_final = torch.cat([img_features, txt_features_final], dim=1)\n",
    "                fused_features_final = self.fusion(combined_final)\n",
    "        else:\n",
    "            fused_features_final = fused_features_initial\n",
    "            category_logits = None\n",
    "            type_logits = None\n",
    "        \n",
    "        logits = self.classifier(fused_features_final)\n",
    "        \n",
    "        if predict_auxiliary or need_predict:\n",
    "            return logits, category_logits, type_logits\n",
    "        else:\n",
    "            return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, questions, answers, answer_categories, question_types = batch\n",
    "        \n",
    "        if self.answer_category2id is not None:\n",
    "            ac_indices = torch.tensor([\n",
    "                self.answer_category2id.get(ac, 0) for ac in answer_categories\n",
    "            ], dtype=torch.long, device=images.device)\n",
    "        else:\n",
    "            ac_indices = torch.zeros(len(answer_categories), dtype=torch.long, device=images.device)\n",
    "        \n",
    "        if self.question_type2id is not None:\n",
    "            qt_indices = torch.tensor([\n",
    "                self.question_type2id.get(qt, 0) for qt in question_types\n",
    "            ], dtype=torch.long, device=images.device)\n",
    "        else:\n",
    "            qt_indices = torch.zeros(len(question_types), dtype=torch.long, device=images.device)\n",
    "        \n",
    "        logits, category_logits, type_logits = self(\n",
    "            images, questions, answer_categories, question_types,\n",
    "            predict_auxiliary=True\n",
    "        )\n",
    "        \n",
    "        loss_answer = F.cross_entropy(logits, answers)\n",
    "        loss_category = F.cross_entropy(category_logits, ac_indices)\n",
    "        loss_type = F.cross_entropy(type_logits, qt_indices)\n",
    "        \n",
    "        total_loss = loss_answer + self.category_loss_weight * loss_category + self.type_loss_weight * loss_type\n",
    "        \n",
    "        batch_size = answers.shape[0]\n",
    "        self.train_acc(logits, answers)\n",
    "        self.log(\"train_loss\", total_loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(\"train_loss_answer\", loss_answer, on_step=True, on_epoch=True, batch_size=batch_size)\n",
    "        self.log(\"train_loss_category\", loss_category, on_step=True, on_epoch=True, batch_size=batch_size)\n",
    "        self.log(\"train_loss_type\", loss_type, on_step=True, on_epoch=True, batch_size=batch_size)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, questions, answers, answer_categories, question_types = batch\n",
    "        \n",
    "        logits, category_logits, type_logits = self(\n",
    "            images, questions, answer_categories, question_types,\n",
    "            predict_auxiliary=True\n",
    "        )\n",
    "        \n",
    "        loss = F.cross_entropy(logits, answers)\n",
    "        \n",
    "        if self.answer_category2id is not None:\n",
    "            ac_indices = torch.tensor([\n",
    "                self.answer_category2id.get(ac, 0) for ac in answer_categories\n",
    "            ], dtype=torch.long, device=images.device)\n",
    "            category_acc = (category_logits.argmax(dim=1) == ac_indices).float().mean()\n",
    "            self.log(\"val_category_acc\", category_acc, on_step=False, on_epoch=True, batch_size=answers.shape[0])\n",
    "        \n",
    "        if self.question_type2id is not None:\n",
    "            qt_indices = torch.tensor([\n",
    "                self.question_type2id.get(qt, 0) for qt in question_types\n",
    "            ], dtype=torch.long, device=images.device)\n",
    "            type_acc = (type_logits.argmax(dim=1) == qt_indices).float().mean()\n",
    "            self.log(\"val_type_acc\", type_acc, on_step=False, on_epoch=True, batch_size=answers.shape[0])\n",
    "        \n",
    "        batch_size = answers.shape[0]\n",
    "        self.val_acc(logits, answers)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        self.log(\"val_acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=8,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "def get_submission_final(model, test_df, images_dir, submission_path, id2answer, batch_size=128, num_workers=4):\n",
    "    \"\"\"Простая функция для инференса без DataModule\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.float()\n",
    "    model.eval()\n",
    "    \n",
    "    # Создаём только test датасет\n",
    "    test_dataset = VQADataset(\n",
    "        df=test_df,\n",
    "        images_dir=images_dir,\n",
    "        word2id=None,  # не используется для BERT\n",
    "        answer2id=None,\n",
    "        max_question_len=20,  # не важно для BERT\n",
    "        is_train=False,\n",
    "        use_bert=True,\n",
    "        augment=False,\n",
    "        image_normalization=\"clip\",\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images, questions, ids, _, _ = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Для BERT questions - это список строк\n",
    "            questions_list = list(questions) if isinstance(questions, tuple) else questions\n",
    "            result = model(images, questions_list, [], [])\n",
    "            \n",
    "            if isinstance(result, tuple):\n",
    "                logits = result[0]\n",
    "            else:\n",
    "                logits = result\n",
    "            \n",
    "            pred_ids = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_predictions.extend(pred_ids)\n",
    "            all_ids.extend(ids)\n",
    "    \n",
    "    predicted_answers = [id2answer[pred_id] for pred_id in all_predictions]\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        \"ID\": all_ids,\n",
    "        \"answer\": predicted_answers,\n",
    "    })\n",
    "    \n",
    "    print(f\"Размер submission: {len(submission_df)}\")\n",
    "    print(f\"Колонки: {submission_df.columns.tolist()}\")\n",
    "    print(\"\\nПервые несколько строк:\")\n",
    "    print(submission_df.head(10))\n",
    "    \n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\nSubmission сохранён в {submission_path}\")\n",
    "    \n",
    "    sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "    print(\"Проверка формата:\")\n",
    "    print(f\"  Размер sample_submission: {len(sample_sub)}\")\n",
    "    print(f\"  Размер нашего submission: {len(submission_df)}\")\n",
    "    print(f\"  Совпадают ID: {set(submission_df['ID']) == set(sample_sub['ID'])}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ОСНОВНОЙ ПАЙПЛАЙН\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"ВОСПРОИЗВОДИМЫЙ ПАЙПЛАЙН: Формирование финального submission.csv\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5.1. Загрузка данных\n",
    "print(\"\\n1. Загрузка данных...\")\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "print(f\"   Train: {len(train)} примеров\")\n",
    "print(f\"   Test: {len(test)} примеров\")\n",
    "\n",
    "# 5.2. Подготовка данных\n",
    "print(\"\\n2. Подготовка данных...\")\n",
    "MAX_WORD_COUNT = 10\n",
    "answer2id, id2answer, train_processed = build_answer_vocab(train, min_freq=2, max_word_count=MAX_WORD_COUNT)\n",
    "print(f\"   Размер словаря ответов: {len(answer2id)}\")\n",
    "\n",
    "# Добавляем категории и типы вопросов\n",
    "train_processed['answer_category'] = train_processed['answer'].apply(categorize_answer)\n",
    "train_processed['question_type'] = train_processed['question'].apply(classify_question_type)\n",
    "\n",
    "# Создаём словари для категорий и типов\n",
    "answer_categories_unique = sorted(train_processed[\"answer_category\"].unique())\n",
    "question_types_unique = sorted(train_processed[\"question_type\"].unique())\n",
    "answer_category2id = {cat: idx for idx, cat in enumerate(answer_categories_unique)}\n",
    "question_type2id = {qt: idx for idx, qt in enumerate(question_types_unique)}\n",
    "print(f\"   Категорий ответов: {len(answer_category2id)}\")\n",
    "print(f\"   Типов вопросов: {len(question_type2id)}\")\n",
    "\n",
    "# 5.3. Загрузка модели\n",
    "print(\"\\n3. Загрузка модели...\")\n",
    "print(f\"   Путь к модели: {MODEL_PATH}\")\n",
    "if not MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Модель не найдена: {MODEL_PATH}\")\n",
    "\n",
    "model = VQAClipBERTpred.load_from_checkpoint(\n",
    "    MODEL_PATH,\n",
    "    clip_model_name=\"ViT-B/16\",\n",
    "    bert_model_name=\"distilbert-base-uncased\",\n",
    "    num_answers=len(answer2id),\n",
    "    num_answer_categories=len(answer_category2id),\n",
    "    num_question_types=len(question_type2id),\n",
    "    hidden_dim=1024,\n",
    "    learning_rate=5e-4,\n",
    "    freeze_image_encoder=True,\n",
    "    freeze_bert=True,\n",
    "    use_cross_attention=True,\n",
    "    category_loss_weight=0.45,\n",
    "    type_loss_weight=0.2\n",
    ")\n",
    "\n",
    "# Устанавливаем словари\n",
    "model.answer_category2id = answer_category2id\n",
    "model.question_type2id = question_type2id\n",
    "\n",
    "# 5.4. Инференс и сохранение\n",
    "print(\"\\n4. Инференс и формирование submission.csv...\")\n",
    "get_submission_final(model, test, IMAGES_DIR, SUBMISSION_PATH, id2answer, batch_size=128, num_workers=4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ПАЙПЛАЙН ЗАВЕРШЁН!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
