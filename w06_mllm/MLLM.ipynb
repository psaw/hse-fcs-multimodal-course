{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Former example ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYOnKQTlAiSB"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqe_GWIueBFp"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QFormerConfig:\n",
    "    d_model: int              # скрытый размер query-токенов и выхода\n",
    "    d_kv: int                 # размер каналов памяти A' (key/value до проекций)\n",
    "    n_heads: int              # число голов внимания\n",
    "    K: int                    # число query-токенов (фиксированная длина Q-Prompt)\n",
    "    L: int                    # число блоков (слоёв) Q-Former\n",
    "    dropout: float = 0.1\n",
    "    use_mem_posenc: bool = False  # добавить синусоидальную позиционку к памяти A'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPh3L8GyeMXb"
   },
   "outputs": [],
   "source": [
    "def _split_heads(x: torch.Tensor, n_heads: int) -> torch.Tensor:\n",
    "    # [B,L,D] -> [B,H,L,Dh]\n",
    "    B, L, D = x.shape\n",
    "    Dh = D // n_heads\n",
    "    x = x.view(B, L, n_heads, Dh).transpose(1, 2)  # [B,H,L,Dh]\n",
    "    return x\n",
    "\n",
    "\n",
    "def _merge_heads(x: torch.Tensor) -> torch.Tensor:\n",
    "    # [B,H,L,Dh] -> [B,L,H*Dh]\n",
    "    B, H, L, Dh = x.shape\n",
    "    return x.transpose(1, 2).contiguous().view(B, L, H * Dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koicT75geMU6"
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B,T,D] -> добавляет PE по T к последней размерности D\n",
    "        B, T, D = x.shape\n",
    "        device = x.device\n",
    "        pe = torch.zeros(T, D, device=device)\n",
    "        position = torch.arange(0, T, device=device, dtype=torch.float32).unsqueeze(1)  # [T,1]\n",
    "        div_term = torch.exp(torch.arange(0, D, 2, device=device, dtype=torch.float32) * (-math.log(10000.0) / D))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return x + pe.unsqueeze(0)  # [1,T,D] broadcast на B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84KQoJj0eMRu"
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.drop_attn = nn.Dropout(dropout)\n",
    "        self.drop_res = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # x: [B,L,D]\n",
    "        B, L, D = x.shape\n",
    "        q = _split_heads(self.W_q(x), self.n_heads)  # [B,H,L,Dh]\n",
    "        k = _split_heads(self.W_k(x), self.n_heads)  # [B,H,L,Dh]\n",
    "        v = _split_heads(self.W_v(x), self.n_heads)  # [B,H,L,Dh]\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(D // self.n_heads)  # [B,H,L,L]\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: [B,1,1,L] с 0 для маскируемых позиций (или -inf добавкой)\n",
    "            if attn_mask.dtype == torch.bool or attn_mask.dtype == torch.uint8:\n",
    "                scores = scores.masked_fill(~attn_mask, float(\"-inf\"))\n",
    "            else:\n",
    "                scores = scores + attn_mask\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.drop_attn(attn)\n",
    "        out = torch.matmul(attn, v)  # [B,H,L,Dh]\n",
    "        out = _merge_heads(out)      # [B,L,D]\n",
    "        out = self.W_o(out)\n",
    "        out = self.drop_res(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model_q: int, d_model_mem: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert d_model_q % n_heads == 0, \"d_model_q must be divisible by n_heads\"\n",
    "        self.d_model_q = d_model_q\n",
    "        self.d_model_mem = d_model_mem\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Проекции в общее пространство внимания размером d_model_q\n",
    "        self.W_q = nn.Linear(d_model_q, d_model_q, bias=True)\n",
    "        self.W_k = nn.Linear(d_model_mem, d_model_q, bias=True)\n",
    "        self.W_v = nn.Linear(d_model_mem, d_model_q, bias=True)\n",
    "        self.W_o = nn.Linear(d_model_q, d_model_q, bias=True)\n",
    "        self.drop_attn = nn.Dropout(dropout)\n",
    "        self.drop_res = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q_inp: torch.Tensor,                 # [B,K,d_model_q] — запросы (Q-токены)\n",
    "        mem: torch.Tensor,                   # [B,T,d_model_mem] — память A'\n",
    "        mem_mask: Optional[torch.Tensor] = None  # [B,T] bool/long -> будет преобразована\n",
    "    ) -> torch.Tensor:\n",
    "        B, K, Dq = q_inp.shape\n",
    "        _, T, _ = mem.shape\n",
    "\n",
    "        q = _split_heads(self.W_q(q_inp), self.n_heads)   # [B,H,K,Dh]\n",
    "        k = _split_heads(self.W_k(mem), self.n_heads)     # [B,H,T,Dh]\n",
    "        v = _split_heads(self.W_v(mem), self.n_heads)     # [B,H,T,Dh]\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(Dq // self.n_heads)  # [B,H,K,T]\n",
    "        if mem_mask is not None:\n",
    "            # mem_mask: [B,T] -> [B,1,1,T], True=keep, False=mask\n",
    "            if mem_mask.dtype != torch.bool:\n",
    "                mem_mask = mem_mask != 0\n",
    "            mask = mem_mask.view(B, 1, 1, T)\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)  # по T\n",
    "        attn = self.drop_attn(attn)\n",
    "        out = torch.matmul(attn, v)       # [B,H,K,Dh]\n",
    "        out = _merge_heads(out)           # [B,K,Dq]\n",
    "        out = self.W_o(out)\n",
    "        out = self.drop_res(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.lin2 = nn.Linear(4 * d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.drop(self.lin2(F.gelu(self.lin1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-GMmXfmeMPB"
   },
   "outputs": [],
   "source": [
    "class QFormerBlock(nn.Module):\n",
    "    \"\"\"Один блок: Pre-LN → SelfAttn(Q) → Pre-LN → CrossAttn(Q←A′) → Pre-LN → FFN\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_kv: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln_q1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
    "        self.ln_q2 = nn.LayerNorm(d_model)\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model, d_kv, n_heads, dropout)\n",
    "        self.ln_q3 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q_tokens: torch.Tensor, mem: torch.Tensor, mem_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        # Self-Attn\n",
    "        x = q_tokens + self.self_attn(self.ln_q1(q_tokens))\n",
    "        # Cross-Attn (Q <- A')\n",
    "        x = x + self.cross_attn(self.ln_q2(x), mem, mem_mask)\n",
    "        # FFN\n",
    "        x = x + self.ffn(self.ln_q3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njiXzHs4NhK3"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_kv: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln_q1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
    "        self.ln_q2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        # Self-Attn\n",
    "        x = x + self.self_attn(self.ln_q1(x), x_mask)\n",
    "\n",
    "        # FFN\n",
    "        x = x + self.ffn(self.ln_q2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQ0uA_bNeMMn"
   },
   "outputs": [],
   "source": [
    "class QFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    Минимальный Q-Former:\n",
    "    - обучаемые query-токены: [1,K,d_model]\n",
    "    - L блоков: SelfAttn(Q) → CrossAttn(Q←A′) → FFN\n",
    "    - вход памяти: A′ [B,T,d_kv], опциональная маска [B,T]\n",
    "    - выход: Q-Prompt [B,K,d_model]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: QFormerConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.query_tokens = nn.Parameter(torch.randn(1, cfg.K, cfg.d_model) * 0.02)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [QFormerBlock(cfg.d_model, cfg.d_kv, cfg.n_heads, cfg.dropout) for _ in range(cfg.L)]\n",
    "        )\n",
    "        self.mem_pos = SinusoidalPositionalEncoding(cfg.d_kv) if cfg.use_mem_posenc else None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def num_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        A_prime: torch.Tensor,                    # [B,T,d_kv] — память (аудио-признаки после проекции)\n",
    "        mem_mask: Optional[torch.Tensor] = None   # [B,T] (1/True = keep)\n",
    "    ) -> torch.Tensor:\n",
    "        assert A_prime.dim() == 3 and A_prime.size(-1) == self.cfg.d_kv, \\\n",
    "            f\"A' must be [B,T,{self.cfg.d_kv}]\"\n",
    "        B = A_prime.size(0)\n",
    "        mem = self.mem_pos(A_prime) if self.mem_pos is not None else A_prime\n",
    "\n",
    "        q = self.query_tokens.expand(B, -1, -1)  # [B,K,d_model]\n",
    "        for layer in self.layers:\n",
    "            q = layer(q, mem, mem_mask)          # [B,K,d_model]\n",
    "        return q  # это и есть Q-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls39xGl2eMKB"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Проектор: CLIP(512) -> prefix_length * hidden_size_LM\"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, hid_dim: int = 2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hid_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ClipCapPrefix(nn.Module):\n",
    "    \"\"\"\n",
    "    Frozen LM (GPT-2) + обучаемый projector.\n",
    "    Учим только projector, который превращает CLIP-эмбеддинг в prefix-эмбеддинги для LM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 gpt_model,\n",
    "                 prefix_length: int,\n",
    "                 cfg: QFormerConfig,\n",
    "                 prefix_size: int = 512,\n",
    "\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model\n",
    "        self.prefix_length = prefix_length\n",
    "\n",
    "        # берём слой входных эмбеддингов правильным общим способом\n",
    "        self.wte = self.gpt.get_input_embeddings()\n",
    "        self.hidden = self.wte.embedding_dim  # hidden_size GPT-2\n",
    "\n",
    "        #TODO заменить проекцию на другую реализацию self.project = MLP(prefix_size, prefix_length * self.hidden)\n",
    "        self.project = QFormer(cfg)\n",
    "        # можно использовать трансформер блоки\n",
    "        # self.project = nn.ModuleList([TrasformerBlock() for _ in cfg.L])\n",
    "        # можно использовать свертки\n",
    "\n",
    "\n",
    "        # замораживаем LM: учим только project\n",
    "        for p in self.gpt.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, prefix: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T]\n",
    "        prefix:    [B, T2, Emb]\n",
    "        attention_mask: [B, T]\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "\n",
    "        # эмбеддинги текста: [B, T, H]\n",
    "        text_emb = self.wte(input_ids)\n",
    "\n",
    "        # prefix-эмбеддинги: [B, P, H]\n",
    "        pref = self.project(prefix)\n",
    "\n",
    "        # fusion: concat(prefix, text) -> [B, P+T, H]\n",
    "        inputs_embeds = torch.cat([pref, text_emb], dim=1)\n",
    "\n",
    "        # attention mask тоже расширяем на prefix (там все 1)\n",
    "        prefix_attn = torch.ones((B, self.prefix_length), device=input_ids.device, dtype=attention_mask.dtype)\n",
    "        # [B, P+T]\n",
    "        full_attn = torch.cat([prefix_attn, attention_mask], dim=1)  # [B, P+T]\n",
    "\n",
    "        # labels: prefix игнорируем (-100), текст оставляем как есть.\n",
    "        # transformers labels сдвигаются внутри модели, а -100 маскируется из loss.\n",
    "        labels = torch.cat(\n",
    "            [torch.full((B, self.prefix_length), -100, device=input_ids.device), input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        out = self.gpt(inputs_embeds=inputs_embeds, attention_mask=full_attn, labels=labels)\n",
    "        return out\n",
    "\n",
    "prefix_length = 10\n",
    "clipcap = ClipCapPrefix(gpt_model=gpt, prefix_length=prefix_length).to(device)\n",
    "\n",
    "print(\"Trainable params (должен быть только projector):\",\n",
    "      sum(p.numel() for p in clipcap.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSbjna_TeMGm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bOrVzgPeL9j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHfpog6geLY6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
